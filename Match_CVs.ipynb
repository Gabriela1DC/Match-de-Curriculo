{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Match-CVs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qg4EL1yFk0i"
      },
      "source": [
        "# Match de CVs com vagas de emprego"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3z4-OMjIpEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5a91eb-acd0-437b-c1d2-998511dc96bd"
      },
      "source": [
        "!pip install pdfplumber\n",
        "import pdfplumber\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.5.28.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20200517\n",
            "  Downloading pdfminer.six-20200517-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 22.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Collecting Wand\n",
            "  Downloading Wand-0.6.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 45.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfplumber\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.28-py3-none-any.whl size=32240 sha256=bef14c79f522bbcc67533a523bceafff0d8872bef7848ad2e95a5f5820c31c8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b1/a0/c0a77b756d580f53b3806ae0e0b3ec945a8d05fca1d6e10cc1\n",
            "Successfully built pdfplumber\n",
            "Installing collected packages: pycryptodome, Wand, pdfminer.six, pdfplumber\n",
            "Successfully installed Wand-0.6.6 pdfminer.six-20200517 pdfplumber-0.5.28 pycryptodome-3.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6_JYVTnu6lt"
      },
      "source": [
        "cvPdf = pdfplumber.open('/content/Juliana.pdf')\n",
        "primeira_pagina = cvPdf.pages[0]\n",
        "texto=primeira_pagina.extract_text()\n",
        "texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HwMQgwS0KX5"
      },
      "source": [
        "cvPdf.metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxSTzHZL1fFd"
      },
      "source": [
        "cvPdf.hyperlinks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIgC7t9K1nSV"
      },
      "source": [
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hinUJ_k72KRG"
      },
      "source": [
        "# Pré Processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpRTYTYj2ASZ"
      },
      "source": [
        "nltk.download('punkt')\n",
        "lista_palavras=nltk.tokenize.word_tokenize(texto)\n",
        "lista_palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbx8LmV73A90"
      },
      "source": [
        "lista_palavras=[palavra.lower() for palavra in lista_palavras]\n",
        "lista_palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kGt1MmO3YjD"
      },
      "source": [
        "pontuacao=['(', ')', ',', ':', ';', '-', '_', '%', '@', '[', ']', '{', '}' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5K1fBhL4cDr"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words=nltk.corpus.stopwords.words('portuguese')\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHuhFNlI5dPi"
      },
      "source": [
        "keywords=[palavra for palavra in lista_palavras if not palavra in stop_words and not palavra in pontuacao]\n",
        "keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-u58MuC6KJ5"
      },
      "source": [
        "len(keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO5n1GW26NIZ"
      },
      "source": [
        "textoCv=\" \".join(p for p in keywords)\n",
        "textoCv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjnPU20M6zAo"
      },
      "source": [
        "# Word Cloud - Nuvem de Palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mFxmJVn66NM"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci11Pbf77BJB"
      },
      "source": [
        "wordcloud=WordCloud(\n",
        "    background_color='#f054c9',\n",
        "    width=1280 ,\n",
        "    height=720 ,\n",
        "    max_font_size=150,\n",
        "    colormap= 'Blues'\n",
        ").generate(textoCv)\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(16,9))\n",
        "ax.imshow(wordcloud)\n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)\n",
        "wordcloud.to_file('MYwordcloud.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCxdeu06VYmx"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def NuvemDePalavras(cv, salvar = True):\n",
        "  cvPdf = pdfplumber.open(cv)\n",
        "  primeira_pagina=cvPdf.pages[0]\n",
        "  texto=primeira_pagina.extract_text()\n",
        "\n",
        "\n",
        "  nltk.download('punkt')\n",
        "  lista_palavras=nltk.tokenize.word_tokenize(texto)\n",
        "  lista_palavras=[palavra.lower() for palavra in lista_palavras]\n",
        "\n",
        "  pontuacao=['(', ')', ',', ':', ';', '-', '_', '%', '@', '[', ']', '{', '}' ]\n",
        "  nltk.download('stopwords')\n",
        "  stop_words=nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "  keywords=[palavra for palavra in lista_palavras if not palavra in stop_words and not palavra in pontuacao]\n",
        "  textoCv=\" \".join(p for p in keywords)\n",
        "\n",
        "  wordcloud=WordCloud(\n",
        "    background_color='#f054c9',\n",
        "    width=1280 ,\n",
        "    height=720 ,\n",
        "    max_font_size=150,\n",
        "    colormap= 'Blues').generate(textoCv)\n",
        "\n",
        "  fig,ax = plt.subplots(figsize=(16,9))\n",
        "  ax.imshow(wordcloud)\n",
        "  ax.set_axis_off()\n",
        "  plt.imshow(wordcloud)\n",
        "  wordcloud.to_file('MYwordcloud.png')\n",
        "  plt.show()\n",
        "\n",
        "  if salvar:\n",
        "    wordcloud.to_file('CurriculumVwordcloud.png')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4131s_oRkgmB"
      },
      "source": [
        "NuvemDePalavras('/content/Juliana.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRdrYgRTlA3q"
      },
      "source": [
        "#Vagas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz5T8aRDljiY"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms14BMpclCaI"
      },
      "source": [
        "vagas=pd.read_excel('/content/vagas.xlsx', sheet_name=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8l_d5xl2Lq"
      },
      "source": [
        "n_vagas = len(vagas.keys())\n",
        "n_vagas\n",
        "nome_vagas=list(vagas.keys())\n",
        "nome_vagas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Y2h-0tmbcp"
      },
      "source": [
        "n_vagas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ZbbcUam0ol"
      },
      "source": [
        "vaga=[vagas[nome_vagas[i]] for i in range(n_vagas)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kawr1bRbq9mR"
      },
      "source": [
        "vaga[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtuy1GZ0nYHq"
      },
      "source": [
        "vaga1=vaga[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB1hfxWio6P5"
      },
      "source": [
        "palavras_chaves=list(vaga1['palavras-chave'])\n",
        "palavras_chaves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5E53k67rNFN"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiCMn9j7o6pR"
      },
      "source": [
        "limite = 5\n",
        "  \n",
        "pesos = list(vaga1['pesos'])\n",
        "palavras_chaves = list(vaga1['palavras-chave'])\n",
        "pmax = np.sum(np.array(pesos) * limite)\n",
        "print(pmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5vcXwg9sxck"
      },
      "source": [
        "cont = [textoCv.count(pc) for pc in palavras_chaves]\n",
        "cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNGRzcnys76T"
      },
      "source": [
        "def aux(x):\n",
        "    return x if x <= limite else limite\n",
        "\n",
        "cont = [aux(i) for i in cont]\n",
        "cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXjPClPFtEB3"
      },
      "source": [
        "score = ((np.array(cont) * pesos).sum()/pmax).round(4)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3Bq6XOtF2P"
      },
      "source": [
        "pontuacao=['(', ')', ',', ':', ';', '-', '_', '%', '@', '[', ']', '{', '}' ]\n",
        "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "def MatchCV(cv, vaga, limite = 5):\n",
        "    '''\n",
        "    cv: caminho de um arquivo PDF\n",
        "    vaga: dataset de palavras-chave e pesos\n",
        "    '''\n",
        "\n",
        "    cvPdf = pdfplumber.open(cv)\n",
        "    primeira_pagina = cvPdf.pages[0]  #lê apenas a primeira página\n",
        "    texto = primeira_pagina.extract_text()\n",
        "\n",
        "    lista_de_palavras = nltk.tokenize.word_tokenize(texto)  # transforma o texto cru em uma lista de termos\n",
        "    lista_de_palavras = [palavra.lower() for palavra in lista_de_palavras]  # deixando tudo minusculo\n",
        "\n",
        "    keywords = [palavra for palavra in lista_de_palavras if not palavra in stop_words and not palavra in pontuacao]  # tira as pontuacoes e stopwords \n",
        "    textoCv = \" \".join(s for s in keywords)  # junta tudo em um texto só novamente. \n",
        "\n",
        "    pesos = list(vaga['pesos'])\n",
        "    palavras_chaves = list(vaga['palavras-chave'])\n",
        "\n",
        "    cont = [textoCv.count(pc) for pc in palavras_chaves]  # conta quantas vezes cada termo da vaga aparece no texto do cv\n",
        "\n",
        "    def aux(x, limite):\n",
        "        return x if x <= limite else limite\n",
        "\n",
        "    cont = [aux(i, limite) for i in cont]   # coloca o limite na contagem de palavras\n",
        "\n",
        "    pmax = np.sum(np.array(pesos) * limite) \n",
        "\n",
        "    score = ((np.array(cont) * pesos).sum()/pmax).round(4)\n",
        "\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBrjI0Ztx6D"
      },
      "source": [
        "MatchCV('/content/Juliana.pdf', vaga[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCKQsxZYuPEs"
      },
      "source": [
        "lista_de_vagas = vaga\n",
        "\n",
        "lista_de_cvs = ['/content/Agner.pdf',\n",
        "                '/content/Juliana.pdf',\n",
        "                '/content/Nayara.pdf',\n",
        "                '/content/Victor.pdf',\n",
        "                '/content/Rafael.pdf',\n",
        "                '/content/Sabrina.pdf']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdE8kRqpuTnS"
      },
      "source": [
        "lista_de_vagas[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZccxIgn7ufDO"
      },
      "source": [
        "pessoas = [[MatchCV(cv, vaga) for vaga in lista_de_vagas] for cv in lista_de_cvs]\n",
        "pessoas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KLuxDFHuuie"
      },
      "source": [
        "nomes = [cv.split('/')[-1].split('.')[0] for cv in lista_de_cvs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X8B316NuxzC"
      },
      "source": [
        "matchs = pd.DataFrame(pessoas, columns = nome_vagas, index = nomes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXPeBgixuyfN"
      },
      "source": [
        "matchs.sort_values(by = 'eng_dados', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}